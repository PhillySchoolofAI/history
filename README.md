# Notable Papers:

(there is a lot here, I will try to organize as I am familar with all papers listed below)

* [A Logical Calculus of the Ideas Immanent in Nervous Activity](http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf)

* [The Logic Theory Machine; A Complex Information Processing System](http://shelf1.library.cmu.edu/IMLS/MindModels/logictheorymachine.pdf)

* [The Logic Theory Machine: A Model Heuristic Program](https://history-computer.com/Library/Logic%20Theorist%20memorandum.pdf)

* [Learning representations by back-propogating errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)

* [Computing Machinery and Intelligence](https://www.csee.umbc.edu/courses/471/papers/turing.pdf)

* [A Threshold Selection Method from Gray-Level Histograms](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4310076)

* [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)

* [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

* [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)

* [Induction of Decision Trees](https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf)

* [Large-scale Video Classification with Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf)

* [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)

* [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)

* [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)

* [Deconvolutional Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)

* [Long Short Term Memory](http://www.bioinf.jku.at/publications/older/2604.pdf)

* [Supervised Sequence Labelling with Recurrent Neural Networks](http://www.cs.toronto.edu/~graves/preprint.pdf)

* [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)

* [Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting](https://arxiv.org/abs/1506.04214v1)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555v1)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)

* [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)

* [Zero-bias autoencoders and the benefits of co-adapting features](https://arxiv.org/abs/1402.3337)

* [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289v1)

* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)

* [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)

* [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)

* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)

* [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)

* [Incorporating Nesterov Momentum into Adam](http://cs229.stanford.edu/proj2015/054_report.pdf)

* [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)

* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)

* [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)

* [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)

* [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)

* [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431)

* [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)

* [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)

* [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)

* [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)

* [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)

* [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)

* [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)

* [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)

* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)

* [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

* [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

* [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/abs/1312.6120)

* [Learning to Execute](https://arxiv.org/abs/1312.6120)

* [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

* [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](https://arxiv.org/abs/1502.05698)

* [End-To-End Memory Networks](https://arxiv.org/abs/1503.08895)

* [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/pdf/1706.02677.pdf)

* [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)

* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)


# Notable Figures:

* Norbert Wiener

* Marvin Minsky

* Richard Wallace

# cybernetics

https://medium.com/@jankrikkeChina/cybernetics-explains-what-ai-is-and-what-it-isnt-13b2baec6cca

